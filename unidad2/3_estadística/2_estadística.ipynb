{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo, Markdown, SVG\n",
    "from functools import partial\n",
    "YouTubeVideo_formato = partial(YouTubeVideo, modestbranding=1, disablekb=0,\n",
    "                               width=640, height=360, autoplay=0, rel=0, showinfo=0)\n",
    "\n",
    "display(Markdown(filename='../../preamble.md'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from functools import partial\n",
    "slider_layout = widgets.Layout(width='600px', height='20px')\n",
    "slider_style = {'description_width': 'initial'}\n",
    "IntSlider_nice = partial(widgets.IntSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "FloatSlider_nice = partial(widgets.FloatSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "SelSlider_nice = partial(widgets.SelectionSlider, style=slider_style, layout=slider_layout, continuous_update=False)\n",
    "from scipy.special import erf\n",
    "gaussian_pdf = lambda x, mu=0, s=1: np.exp(-0.5*(x-mu)**2/s**2)/(s*np.sqrt(2*np.pi))\n",
    "gaussian_cdf = lambda x, mu=0, s=1: 0.5 + 0.5*erf((x-mu)/(s*np.sqrt(2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estadística\n",
    "\n",
    "La estadística busca:\n",
    "\n",
    "> Describir fenómenos complejos a partir de observaciones parciales \n",
    "\n",
    "> Inferir propiedades de una población basándonos en una muestra\n",
    "\n",
    "> Usar datos para responder preguntas y tomar decisiones\n",
    "\n",
    "La estadística es:\n",
    "\n",
    "> Disciplina científica dedicada al desarrollo y estudio de métodos para recopilar, analizar y extraer información de los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estadística descriptiva\n",
    "\n",
    "Sea una muestra de datos. Para entenderla mejor podemos empezar describiendola:\n",
    "- ¿Discreto o continuo? ¿No-negativo?\n",
    "- ¿Dónde está localizada? **Media**\n",
    "- ¿Cuál es su disperción? **Varianza**\n",
    "- ¿Son las colas iguales o distintas? **Simetría** (*Skewness*)\n",
    "- ¿Son las colas ligeras o pesadas? **Curtosis** (*Kurtosis*)\n",
    "- ¿Tiene una moda o múltiples modas?\n",
    "- ¿Cuantiles? ¿Percentiles? \n",
    "- ¿Existen *outliers*?\n",
    "\n",
    "Podemos responder estas preguntas usando [estadísticos de resumen](https://docs.scipy.org/doc/scipy/reference/stats.html#summary-statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-11, 10, num=1000)\n",
    "px = 0.7*gaussian_pdf(x, mu=-4, s=2) + 0.3*gaussian_pdf(x, mu=3, s=2)\n",
    "N = 1000; \n",
    "np.random.seed(0)\n",
    "hatx = np.concatenate((-4 + 2*np.random.randn(int(0.7*N)), \n",
    "                       (3 + 2*np.random.randn(int(0.3*N)))))\n",
    "\n",
    "scipy.stats.describe(hatx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ciertos casos podemos responder estas preguntas graficamente usando:\n",
    "\n",
    "### Histograma\n",
    "\n",
    "- Es una representación numérica de una distribución\n",
    "- Nos permite visualizar las características de la distribución\n",
    "- Se construye dividiendo el dominio en **bines** y contando los datos que caen en cada **bin**\n",
    "- Está definido por la posición y tamaño de los bines\n",
    "- Método no-paramétrico para representar distribuciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "def update_plot(nbins): \n",
    "    ax.cla()\n",
    "    ax.plot(x, px, 'k-', linewidth=4, alpha=0.8)\n",
    "    hist, bin_edges = np.histogram(hatx, bins=nbins, density=True)\n",
    "    ax.bar(bin_edges[:-1], hist, width=bin_edges[1:] - bin_edges[:-1], \n",
    "           edgecolor='k', align='edge', alpha=0.8)\n",
    "    ax.set_xlabel('x')\n",
    "    \n",
    "widgets.interact(update_plot, nbins=SelSlider_nice(options=[2, 5, 10, 15, 20, 50], value=5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel density estimation (KDE)\n",
    "\n",
    "Mismo objetivo que el histograma pero\n",
    "1. Cada dato es \"su propio bin\"\n",
    "1. Los bines se pueden traslapar\n",
    "1. No se escoge la posición o fronteras de bines, solo su ancho\n",
    "\n",
    "Para un set de observaciones unidimensionales $\\{x_i\\}_{i=1,\\ldots, N}$ con dominio continuo\n",
    "\n",
    "$$\n",
    "\\hat f_h(x) = \\frac{1}{Nh} \\sum_{i=1}^N \\kappa \\left ( \\frac{x - x_i}{h} \\right)\n",
    "$$\n",
    "\n",
    "donde \n",
    "- $h$ es el **ancho de banda del kernel** o **tamaño de kernel**\n",
    "- $\\kappa(u)$ es una **función de kernel** que debe ser positiva, con media cero e integrar a la unidad\n",
    "\n",
    "Por ejemplo  el famoso kernel Gaussiano\n",
    "\n",
    "$$\n",
    "\\kappa(u) = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left ( - \\frac{u^2}{2} \\right),\n",
    "$$\n",
    "\n",
    "Ojo: \n",
    "- Usar un kernel gaussiano no es lo mismo que asumir que los datos se distribuyen gaussianos\n",
    "- Se puede usar un kernel gaussiano en datos que no se distribuyen gaussianos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.kde import KernelDensity\n",
    "plt.close('all'); fig, ax = plt.subplots(figsize=(7, 4))\n",
    "ax.plot(x, px, 'k-', linewidth=4, alpha=0.8)\n",
    "ax.scatter(hatx, np.zeros_like(hatx), marker='+', c='k', s=20, alpha=0.1)\n",
    "ax.set_xlabel('x')\n",
    "line_kde = ax.plot(x, np.zeros_like(x))\n",
    "#hs = 0.9*np.std(hatx)*N**(-1/5)\n",
    "def update(k): \n",
    "    kde = scipy.stats.gaussian_kde(hatx, bw_method=lambda kde: k*kde.silverman_factor() )\n",
    "    line_kde[0].set_ydata(kde.pdf(x))\n",
    "    \n",
    "widgets.interact(update, k=SelSlider_nice(description=\"$k=h/h_s$\", options=[1/8, 1/4, 1/2, 1, 2, 4], value=1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más opciones de kernels en [`sklearn.neighbors.KernelDensity`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html#sklearn.neighbors.KernelDensity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia estadística\n",
    "\n",
    "> Extraer conclusiones a partir de hechos a través de una premisa científica\n",
    "\n",
    "- Hechos: Datos\n",
    "- Premisa: Modelo probabilístico\n",
    "- Conclusión: Una cantidad no observada que es interesante\n",
    "\n",
    "> Cuantificar la incerteza de la conclusión dado los datos y el modelo \n",
    "\n",
    "Tareas inferenciales\n",
    "1. Ajustar un modelo: **Máxima verosimilitud**\n",
    "1. Verificar el modelo: **Intervalo de confianza**\n",
    "1. Responder una pregunta usando el modelo: **Test de hipótesis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de modelos paramétricos\n",
    "\n",
    "Se refiere a aquellos modelos que explicitan una distribución de probabilidad\n",
    "\n",
    "### Ejemplo: La pesa defectuosa\n",
    "\n",
    "- Mi pesa está defectuosa\n",
    "- Luego de comer mido $M$ veces mi peso obteniendo un conjunto de observaciones $\\{x_i\\}$\n",
    "- El objetivo es encontrar mi peso real $\\hat x$. \n",
    "\n",
    "Puedo modelar mis observaciones como\n",
    "$$\n",
    "x_i = \\hat x + \\varepsilon_i\n",
    "$$\n",
    "donde $\\varepsilon_i$ corresponde al ruido del instrumento\n",
    "\n",
    "Asumamos que $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$\n",
    "\n",
    "es decir que el ruido es\n",
    "- Independiente\n",
    "- Gaussiano\n",
    "- Con media cero y varianza $\\sigma_\\varepsilon^2$\n",
    "\n",
    "Con esto podemos escribir la probabilidad de observar $x_i$ dado un cierto valor $\\hat x$ como\n",
    "\n",
    "$$\n",
    "p(x_i|\\hat x) = \\mathcal{N}(\\hat x, \\sigma_\\varepsilon^2)\n",
    "$$\n",
    "\n",
    "> Mi modelo tiene dos parámetros: $\\hat x$ y $\\sigma_\\varepsilon^2$\n",
    "\n",
    "Si **asumimos** que las observaciones son independientes e identicamente distribuidas (iid), podemos escribir la probabilidad de observar el conjunto completo como \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "p(x_1, x_2, \\ldots, x_M| \\hat x) &= \\prod_{i=1}^M p(x_i|\\hat x) \\nonumber \\\\\n",
    "&= \\prod_{i=1}^M  \\frac{1}{\\sqrt{2\\pi}\\sigma_\\varepsilon}  \\exp \\left ( - \\frac{1}{2\\sigma_\\varepsilon^2} (x_i - \\hat x)^2 \\right) = \\mathcal{L}(\\hat x)  \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> Esto se conoce como la **verosimilitud** de los parámetros dado el conjunto de observaciones\n",
    "\n",
    "Podemos buscar los parámetros que hacen mi modelo más \"creible\" **maximizando la verosimilitud**\n",
    "\n",
    "\n",
    "$$\n",
    "\\max_{\\hat x}  \\mathcal{L}(\\hat x) = \\max_{\\hat x}  \\log \\mathcal{L}(\\hat x) = -\\frac{1}{2} \\log(2\\pi\\sigma_\\varepsilon^2)M - \\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M (x_i - \\hat x )^2\n",
    "$$\n",
    "\n",
    "Como el logaritmo es monotónico conviene maximizar el log de la verosimilitud\n",
    "\n",
    "Si derivamos e igualamos a cero\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "-\\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M 2(x_i - \\hat x ) &= 0 \\nonumber \\\\\n",
    "\\hat x &= \\frac{1}{M} \\sum_{i=1}^M x_i \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "> El estimador de máxima verosimilitud para la media de una Gaussiana es el clásico promedio muestral\n",
    "\n",
    "Si hicieramos lo mismo para la varianza encontraríamos\n",
    "\n",
    "$$\n",
    "\\sigma_\\epsilon^2 = \\frac{1}{M} \\sum_{i=1}^M (x_i - \\hat x)^2\n",
    "$$\n",
    "\n",
    "> Que es la varianza muestreal (sesgada)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimación de máxima verosimilitud\n",
    "\n",
    "El procedimiento que acabamos de ver se llama *maximum likelihood estimation* (MLE)\n",
    "\n",
    "Procedimiento\n",
    "\n",
    "1. Definir los supuestos del problema, el modelo (distribución) y sus parámetros $\\theta$\n",
    "1. Escribir el **logaritmo de la verosimilud** de los parámetros $\\log \\mathcal{L}(\\theta)$\n",
    "1. Encontrar $\\theta$ que maximiza \n",
    "$$\n",
    "\\hat \\theta = \\text{arg}\\max_\\theta \\log \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "\n",
    "Las distribuciones de  `scipy.stats` tienen los métodos\n",
    "- `fit (data)` Ajusta una distribución continua a datos usando MLE\n",
    "- `expect (func)` Valor esperado de una función c/r a la distribución\n",
    "- `interval (alpha)` Cotas para el intervalo que contiene un porcentaje $\\alpha$ de la distribución\n",
    "\n",
    "Para distribuciones complicadas sin solución analítica se pueden usar métodos iterativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Ajustando y comparando distintas distribuciones\n",
    "\n",
    "Observemos la siguiente distribución, ¿Qué características resaltan? ¿Qué distribución sería apropiado ajustar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "data_set = datasets.load_breast_cancer()\n",
    "x, y = data_set['data'][:, 0], data_set['target']\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.hist(x, bins=20, density=True)\n",
    "ax.set_xlabel('Tamaño del nodulo');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos probar varias distribuciones y observar el resultado\n",
    "\n",
    "¿Cómo medir la bondad del ajuste?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = np.linspace(np.amin(x), np.amax(x), num=500)\n",
    "dist = scipy.stats.uniform\n",
    "params = dist.fit(x)\n",
    "print(dist.name)\n",
    "print(params)\n",
    "p_plot = dist(*params[:-2], loc=params[-2], scale=params[-1]).pdf(x_plot)\n",
    "ax.plot(x_plot, p_plot, label=dist.name)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bondad de ajuste\n",
    "\n",
    "Podemos usar el test chi-cuadrado, el [test de Akaike](https://en.wikipedia.org/wiki/Akaike_information_criterion), el test no-paramétrico de Kolmogorov-Smirnov (KS) o gráficos QQ para medir que tan bien se ajusta nuestra distribución teórica a los datos\n",
    "\n",
    "El test KS nos permite comparar que tan distinta es una distribución continua empírica de una teórica comparando sus CDFs: [`scipy.stats.kstest`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html)\n",
    "\n",
    "- Require que los datos estén estandarizados \n",
    "\n",
    "$$\n",
    "Z = \\frac{X - \\mu_X}{\\sigma_X}\n",
    "$$\n",
    "\n",
    "- Retorna un estadístico: Mientras más cerca a cero, mejor es el ajuste\n",
    "- También retorna un p-value: Si es menor que $\\alpha$ entonces rechazo la hipótesis nula de que las distribuciones son iguales con un $1-\\alpha$ de confianza. Veremos test de hipótesis más adelante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_std = (x-np.mean(x))/np.std(x)\n",
    "ks_res = []\n",
    "for dist in [scipy.stats.norm, scipy.stats.lognorm, scipy.stats.beta, scipy.stats.gamma, scipy.stats.genextreme]:    \n",
    "    params = dist.fit(x_std)\n",
    "    fitted_dist = dist(*params[:-2], loc=params[-2], scale=params[-1])\n",
    "    ks_res.append(scipy.stats.kstest(rvs=x_std, cdf=fitted_dist.cdf))\n",
    "    print(dist.name)\n",
    "    print(ks_res[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respondiendo preguntas usando nuestro modelo\n",
    "\n",
    "1. ¿Cuál es la media de la distribución? \n",
    "1. ¿Cúal es la probabilidad de que el \"tamaño del nódulo\" sea mayor o igual que 20?\n",
    "1. ¿Cúal es el tamaño de nódulo que acumula el 90% de la distribución?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.hist(x, bins=20, density=True)\n",
    "ax.set_xlabel('Tamaño del nodulo')\n",
    "dist = scipy.stats.genextreme\n",
    "params = dist.fit(x)\n",
    "fitted_dist = dist(*params[:-2], loc=params[-2], scale=params[-1])\n",
    "p_plot = fitted_dist.pdf(x_plot)\n",
    "ax.plot(x_plot, p_plot, c='k', lw=2);\n",
    "ax.fill_between(x_plot[x_plot>=20], np.zeros_like(x_plot[x_plot>=20]), p_plot[x_plot>=20], \n",
    "                alpha=0.5, color='k', zorder=20)\n",
    "\n",
    "# P1: Usando el atributo mean() \n",
    "display(fitted_dist.mean())\n",
    "# P2: Usando el atributo CDF\n",
    "display(1. - fitted_dist.cdf(20))\n",
    "# P3: Usando el atributo PPF\n",
    "display(fitted_dist.ppf(0.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p-values: https://ipython-books.github.io/72-getting-started-with-statistical-hypothesis-testing-a-simple-z-test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.hist(x[y==0], bins=30, range=(5, 30), density=False, alpha=0.75, label='maligno')\n",
    "ax.hist(x[y==1], bins=30, range=(5, 30), density=False, alpha=0.75, label='benigno')\n",
    "ax.set_xlabel('Tamaño del nodulo');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Clasificador Bayesiano Ingenuo\n",
    "\n",
    "Tenemos un dataset de pacientes con cancer de mama \n",
    "\n",
    "Cada paciente está representado por\n",
    "- x: radio de la muestra (continua)\n",
    "- z: textura de la muestra (continua)\n",
    "- y: etiqueta de la muestra (0:maligno o 1:benigno)\n",
    "\n",
    "El dataset tiene 569 pacientes, 212 con tumores malignos(0) y 357 tumores benignos (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = data_set['data'][:, :2], data_set['target']\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "ax.scatter(x[y==0, 0], x[y==0, 1], c='k', s=10, marker='o', label='Sanos', alpha=0.5)\n",
    "ax.scatter(x[y==1, 0], x[y==1, 1], c='k', s=10, marker='x', label='Cancer', alpha=0.5)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el teorema de Bayes podemos escribir la probabilidad de que el tumor sea benigno dada sus características (*posterior*) como\n",
    "\n",
    "$$\n",
    "p(y=1|x, z) = \\frac{p(x, z|y=1) p(y=1)}{p(x, z)}\n",
    "$$\n",
    "\n",
    "y la probabilidad de que sea maligno como\n",
    "$$\n",
    "p(y=0|x, z) = \\frac{p(x, z|y=0) p(y=0)}{p(x, z)}\n",
    "$$\n",
    "\n",
    "- Nos interesa saber si $p(y=0|x, z) > p(y=1|x, z)$\n",
    "- La evidencia $p(x, z)$ es difícil de estimar \n",
    "\n",
    "Entonces\n",
    "\n",
    "$$\n",
    "\\frac{p(y=1|x, z)}{p(y=0|x, z)} = \\frac{p(x, z|y=1) p(y=1)}{p(x, z|y=0) p(y=0)}\n",
    "$$\n",
    "\n",
    "y los *priors*: $p(y=0) = \\frac{212}{569} \\approx 0.41$, $p(y=1) = \\frac{357}{569} \\approx 0.59$\n",
    "\n",
    "Lo único que falta es la verosimilitud, asumiremos que\n",
    "\n",
    "- las características son condicionalmente independientes\n",
    "$$\n",
    "p(x, z|y) = p(x|y) p(z|y)\n",
    "$$\n",
    "- las características son normales\n",
    "$$\n",
    "p(x|y) = \\mathcal{N}(\\mu_x, \\sigma_x^2)\n",
    "$$\n",
    "\n",
    "Estos supuestos son lo que hacen al clasificador \"ingenuo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probabilidades a priori\n",
    "from collections import Counter\n",
    "py = [Counter(y)[i]/len(y) for i in range(2)]\n",
    "# Ajuste de verosimilitudes\n",
    "dists = {}\n",
    "for y_ in [0, 1]: # Para cada clase\n",
    "    for x_ in [0, 1]: # para cada característica\n",
    "        params = scipy.stats.norm.fit(x[y==y_, x_])\n",
    "        dists[(y_, x_)] = scipy.stats.norm(loc=params[-2], scale=params[-1])\n",
    "\n",
    "def likelihoods(x, z):\n",
    "    pxzy0 = dists[(0, 0)].pdf(x)*dists[(0, 1)].pdf(z)\n",
    "    pxzy1 = dists[(1, 0)].pdf(x)*dists[(1, 1)].pdf(z)\n",
    "    return pxzy0, pxzy1, (pxzy1*py[1])/(pxzy0*py[0])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 3), tight_layout=True)\n",
    "for k, (label, marker) in enumerate(zip(['Sanos', 'Cancer'], ['o', 'x'])):\n",
    "    ax.scatter(x[y==k, 0], x[y==k, 1], c='k', s=10, \n",
    "               marker=marker, label=label, alpha=0.5)\n",
    "\n",
    "x_plot = np.linspace(np.amin(x[:, 0]), np.amax(x[:, 0]), num=500)\n",
    "z_plot = np.linspace(np.amin(x[:, 1]), np.amax(x[:, 1]), num=500)\n",
    "X, Z = np.meshgrid(x_plot, z_plot)\n",
    "Y = likelihoods(X, Z)\n",
    "ax.contourf(X, Z, Y[1] - Y[0], zorder=-1, cmap=plt.cm.RdBu, \n",
    "            vmin=-2e-2, vmax=2e-2, levels=20)\n",
    "ax.set_xlim([np.amin(x_plot), np.amax(x_plot)])\n",
    "ax.set_ylim([np.amin(z_plot), np.amax(z_plot)])\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Z');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decidimos entre sano y enfermo usando el cociente entre los posterior\n",
    "\n",
    "$$\n",
    "\\frac{p(y=1|x, z)}{p(y=0|x, z)} > R\n",
    "$$\n",
    "\n",
    "Dos tipos de errores:\n",
    "1. Predecir que está enfermo $\\hat y=1$ cuando en realidad estaba sano $y=0$\n",
    "1. Predecir que está sano $\\hat y=0$ cuando en realidad estaba enfermo $y=1$\n",
    "\n",
    "¿Las decisiones tienen igual riesgo ($R$=1) ? \n",
    "\n",
    "Podemos \"calibrar\" la importancia de las decisiones usando un $R$ mayor o menor que $1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.contour(X, Z, Y[2] > 1., zorder=-1, levels=[0]);\n",
    "#ax.contour(X, Z, Y[2] > 0.5, zorder=-1, levels=[0]);\n",
    "#ax.contour(X, Z, Y[2] > 0.1, zorder=-1, levels=[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resumen\n",
    "- Cada clase es representada por una distribución Gaussiana\n",
    "- Entrenamos cada Gaussiana por **máxima verosimilitud**\n",
    "- El cociente entre los posterior nos da la clase más probable\n",
    "- **Ventajas:** Fácil de entrenar, en general no se sobreajusta\n",
    "- **Desventajas:** Supuestos fuertes, características independientes y probabilidades normales\n",
    "- Podemos entrenar un clasificador para datos discretos si usamos distribuciones multinomiales\n",
    "- Implementación en [scikit-learn](https://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [Más sobre clasificador bayesiano](https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Ajustando una recta\n",
    "\n",
    "Sea el siguiente dataset de [consumo de helados](https://forge.scilab.org/index.php/p/rdataset/source/tree/master/csv/Ecdat/Icecream.csv) en USA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc -c https://forge.scilab.org/index.php/p/rdataset/source/file/master/csv/Ecdat/Icecream.csv\n",
    "df = pd.read_csv('Icecream.csv', header=0, index_col=0)\n",
    "df.columns = ['consumo', 'ingreso', 'precio', 'temperatura']\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    ax[i].scatter(df[col], df[\"consumo\"])\n",
    "    ax[i].set_xlabel(col)\n",
    "ax[0].set_ylabel(df.columns[0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sea $y$ el consumo y $x$ la temperatura.\n",
    "\n",
    "Asumiendo errores gaussianos\n",
    "\n",
    "$$\n",
    "y_i = \\hat y_i + \\epsilon_i, \\epsilon_i \\sim \\mathcal{N}(0, \\sigma^2),\n",
    "$$\n",
    "\n",
    "un modelo lineal de dos parámetros,\n",
    "\n",
    "$$\n",
    "\\hat y_i = \\theta_0 + \\theta_1 x_i\n",
    "$$\n",
    "\n",
    "y observaciones iid, podemos estimar $\\theta$ buscando la máxima verosimilitud del modelo\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^M (y_i - \\theta_0 - \\theta_1 x_i)^2\n",
    "$$\n",
    "\n",
    "donde \n",
    "$$\n",
    "\\sum_i y_i  - M\\theta_0 - \\theta_1  \\sum_i x_i = 0 \\rightarrow \\theta_0 = \\bar y - \\theta_1 \\bar x\n",
    "$$\n",
    "$$\n",
    "\\sum_i y_i x_i - \\theta_0 \\sum_i x_i - \\theta_1 \\sum_i x_i^2 = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta_1 = \\frac{\\sum_i x_i y_i - M \\bar x \\bar y}{\\sum_i x_i^2 - M \\bar x^2} = \\frac{ \\sum_i (y_i - \\bar y)(x_i - \\bar x)}{\\sum_i (x_i - \\bar x)^2} = \\frac{\\text{COV}(x, y)}{\\text{Var}(x)}\n",
    "$$\n",
    "\n",
    "la fuerza de la correlación se mide con \n",
    "\n",
    "$$\n",
    "r^2 = 1 - \\frac{\\sum_i ( y_i - \\hat y_i)^2}{\\sum_i ( y_i - \\bar y)^2} = 1 - \\frac{\\frac{1}{M} \\sum_i (y_i - \\hat y_i)^2}{\\text{Var}(y)} = \\frac{\\text{COV}^2(x, y)}{\\text{Var}(x) \\text{Var}(y)}\n",
    "$$\n",
    "\n",
    "donde $r = \\frac{\\text{COV}(x, y)}{\\sqrt{\\text{Var}(x) \\text{Var}(y)}} \\in [-1, 1]$ se conoce como [coeficiente de correlación de Pearson](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)\n",
    "\n",
    "Podemos usar pandas para calcular $r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es decir que existe \n",
    "- una correlación positiva alta entre consumo y temperatura\n",
    "- una correlación negativa moderada entre consumo y precio\n",
    "- una correlación cercana a cero entre consumo e ingreso\n",
    "\n",
    "Podemos usar la función [`scipy.stats.linregress`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html) para recuperar $\\theta_0$, $\\theta_1$ y $r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "ax[0].set_ylabel(df.columns[0]);\n",
    "\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    ax[i].scatter(df[col], df[\"consumo\"])    \n",
    "    res = scipy.stats.linregress(df[col], df[\"consumo\"])\n",
    "    ax[i].set_title(\"r: {0:0.5f}\".format(res.rvalue))\n",
    "    x_plot = np.linspace(np.amin(df[col]), np. amax(df[col]), num=100)\n",
    "    ax[i].plot(x_plot, res.slope*x_plot + res.intercept, lw=4, alpha=0.5, c='k');\n",
    "    ax[i].set_xlabel(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Respondiendo preguntas con nuestro modelo\n",
    "\n",
    "1. ¿Qué tan confiables son los valores de $\\theta_0$, $\\theta_1$ y $r$?\n",
    "1. ¿Son las correlaciones encontradas significativas?\n",
    "\n",
    "\n",
    "Dos caminos para responder\n",
    "- Pruebas paramétricas\n",
    "- Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test de hipótesis\n",
    "\n",
    "Se aplica un tratamiento nuevo a una muestra de la población \n",
    "- ¿Es el tratamiento efectivo?\n",
    "- ¿Existe una diferencia entre los que tomaron el tratamiento y los que no?\n",
    "\n",
    "El test de hipótesis es un procedimiento estadístico para comprobar si el resultado de un experimento es significativo en la población\n",
    "\n",
    "Para esto formulamos dos escenarios cada uno con una hipótesis asociada\n",
    "- Hipótesis nula ($H_0$): El experimento no produjo diferencia. El experimento no tuvo efecto. Las observaciones son producto del azar\n",
    "- Hipótesis alternativa ($H_A$): Usualmente el complemento de $H_0$\n",
    "\n",
    "El test de hipótesis se diseña para medir que tan fuerte es la evidencia **en contra** de la hipótesis nula\n",
    "\n",
    "El algoritmo general es de un test de hipótesis:\n",
    "1. Definimos $H_0$ y $H_A$\n",
    "1. Definimos un estadístico $T$\n",
    "1. Asumimos una distribución para $T$ dado que $H_0$ es cierto\n",
    "1. Seleccionamos un nivel de significancia $\\alpha$ \n",
    "1. Calculamos el $T$ para nuestros datos $T_{data}$\n",
    "1. Calculamos el **p-value**: Probabilidad de observar un valor de $T$ más extremo que el observado \n",
    "    - Si nuestro test es de una cola:\n",
    "        - Superior: $p = P(T>T_{data})$\n",
    "        - Inferior: $p = P(T<T_{data})$\n",
    "    - Si nuestro test es dos colas: $p = P(T>T_{data}) + P(T<T_{data})$\n",
    "\n",
    "Finalmente:\n",
    "\n",
    "`Si`  $p < \\alpha$\n",
    "    \n",
    "    Rechazamos la hipótesis nula con confianza (1-\\alpha)\n",
    "De lo contrario:\n",
    "    \n",
    "    No hay suficiente evidencia para rechazar la hipótesis nula\n",
    "    \n",
    "**Escenarios:**\n",
    "- Rechazamos $H_0$ cuando en realidad era cierta (falso positivo): **Error tipo I**\n",
    "    - Ocurre con probabilidad $\\alpha$ (es controlable)\n",
    "- No rechazamos $H_0$ cuando en realidad era falsa (falso negativo): **Error tipo II**\n",
    "    - Ocurre con probabilidad $\\beta$\n",
    "\n",
    "\n",
    "**Errores de interpretación comunes:**\n",
    "\n",
    "- El p-value **no es** la probabilidad de que $H_0$ sea cierta\n",
    "\n",
    "    $p = P(T> T_{data} | H_0) \\neq P(H_0 | T> T_{data})$\n",
    "    \n",
    "- No rechazar $H_0$ no es lo mismo que aceptarla\n",
    "- Rechazar $H_0$ no es lo mismo que aceptar $H_A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: t-test de una muestra \n",
    "\n",
    "Para un conjunto de $M$ observaciones iid $X = {x_1, x_2, \\ldots, x_M}$ con media muestral $\\bar x = \\sum_{i=1}^M x_i$ \n",
    "\n",
    "El t-test de una muestra es un test de hipótesis que busca probar si $\\bar x$ es significativamente distinta de la **media poblacional** $\\mu$, en el caso de que **no conocemos la varianza poblacional** $\\sigma^2$\n",
    "\n",
    "Las hipótesis son\n",
    "\n",
    "- $H_0:$ $\\bar x = \\mu$\n",
    "- $H_A:$ $\\bar x \\neq \\mu$ (dos colas)\n",
    "\n",
    "El estadístico de prueba es \n",
    "\n",
    "$$\n",
    "t = \\frac{\\bar x - \\mu}{\\hat \\sigma /\\sqrt{M-1}}\n",
    "$$\n",
    "\n",
    "donde $\\hat \\sigma = \\sqrt{ \\frac{1}{M} \\sum_{i=1}^M (x_i - \\bar x)^2}$ es la desviación estándar muestral (sesgada)\n",
    "\n",
    "Si asumimos que $\\bar x$ se distribuye $\\mathcal{N}(\\mu, \\frac{\\sigma^2}{M})$ entonces\n",
    "$t$ que se distribuye [t-student](https://en.wikipedia.org/wiki/Student%27s_t-distribution) con $M-1$ grados de libertad\n",
    "\n",
    "- Para muestras iid y $M$ grande el supuesto se cumple por teorema central del límite\n",
    "- Si $M$ es pequeño debemos verificar la normalidad de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-test para probar que la regresión es significativa\n",
    "\n",
    "En este caso queremos hacer el siguiente test\n",
    "- $H_0:$ La pendiente es nula $\\theta_1= 0$ \n",
    "- $H_A:$ La pendiente no es nula: $\\theta_1\\neq 0$ (dos colas)\n",
    "\n",
    "Asumimos que $\\theta_1$ es normal pero que desconocemos su varianza \n",
    "\n",
    "Entonces se puede formular el siguiente estadístico de prueba \n",
    "\n",
    "$$\n",
    "t = \\frac{(\\theta_1-\\theta^*) }{\\text{SE}_{\\theta_1}/\\sqrt{M-2}} = \\frac{ r\\sqrt{M-2}}{\\sqrt{1-r^2}},\n",
    "$$\n",
    "\n",
    "donde en este caso particular se usa $\\theta^*=0$ y $\\text{SE}_{\\theta_1} = \\sqrt{ \\frac{\\frac{1}{M} \\sum_i (y_i - \\hat y_i)^2}{\\text{Var}(x)}}$\n",
    "\n",
    "$t$ se [t-student](https://en.wikipedia.org/wiki/Student%27s_t-distribution) con dos grados de libertad (modelo de dos parámetros) \n",
    "\n",
    "La función `scipy.stats.linregress` implementa este test y retorna los p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(8, 3), tight_layout=True, sharey=True)\n",
    "t = np.linspace(-7, 7, num=1000)\n",
    "M = df.shape[0]\n",
    "ax[0].set_ylabel(df.columns[0]);\n",
    "\n",
    "for i, col in enumerate(df.columns[1:]):\n",
    "    res = scipy.stats.linregress(df[col], df[\"consumo\"])\n",
    "    t_data = res.rvalue*np.sqrt(M-2)/np.sqrt(1.-res.rvalue**2)\n",
    "    display(res)\n",
    "    ax[i].set_title(\"t_data: {0:0.5f}\".format(t_data))\n",
    "    ax[i].set_xlabel(col)\n",
    "    dist = scipy.stats.t(loc=0, scale=1, df=M-2)\n",
    "    p = dist.pdf(t)\n",
    "    ax[i].plot(t, p)\n",
    "    ax[i].plot([dist.ppf(0.025)]*2, [0, np.amax(p)], 'k--')\n",
    "    ax[i].plot([dist.ppf(0.975)]*2, [0, np.amax(p)], 'k--')\n",
    "    ax[i].plot([t_data]*2, [0, np.amax(p)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué podemos decir de las correlaciones con el consumo de helados?\n",
    "\n",
    "Para temperatura y considerando $\\alpha = 0.05$:\n",
    "\n",
    "> $\\text{p-value} = 4.7892e-07 < \\alpha$\n",
    "\n",
    "Por ende rechazamos $H_0$ con 95% de confianza\n",
    "\n",
    "En cambio para ingreso y precio no podemos rechazar $H_0$\n",
    "\n",
    "### Reflexión\n",
    "- ¿Cómo se escogen el estadístico y la distribución de prueba? Depende del problema \n",
    "- ¿Qué prueba puedo usar si quiero hacer regresión lineal multivariada? [ANOVA](https://pythonfordatascience.org/anova-python/)\n",
    "- ¿Qué pasa si mis datos tienen una relación que no es lineal? La prueba no es confiable\n",
    "- ¿Qué pasa si $\\theta_1$ no es normal? La prueba no es confiable\n",
    "- ¿Qué pasa si el ruido no es Gaussiano? La prueba no es confiable\n",
    "- ¿Qué pasa si el ruido es Gaussiano pero su varianza cambia en el tiempo? La prueba no es confiable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba no-paramétrica: *Bootstrap*\n",
    "\n",
    "Podemos estimar la incerteza de un estimador de forma no-paramétrica usando **muestreo tipo *bootstrap***\n",
    "\n",
    "Tomamos nuestros conjunto de datos de tamaño $M$ y creamos $T$ nuevos conjuntos que \"se le parezcan\" \n",
    "\n",
    "Luego se calcula el valor del estimador en esos nuevos conjuntos\n",
    "\n",
    "*Bootstrap* está basado en la Ley de los grandes números y Teorema central del límite \n",
    "\n",
    "<img src=\"https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2016/10/bootstrap-sample.png\">\n",
    "\n",
    "- En este caso supondremos independencia: **Muestreo con reemplazo de tamaño $M$**\n",
    "- También existe bootstrap basado en los residuos y bootstrap dependiente\n",
    "\n",
    "Podemos usar la función [`numpy.random.choice`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html) para generar los nuevos índices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all'); fig, ax = plt.subplots(2, 2, figsize=(6, 5), tight_layout=True)\n",
    "x, y = df[\"temperatura\"].values, df[\"consumo\"].values\n",
    "x_plot = np.linspace(np.amin(x), np. amax(x), num=100)\n",
    "res_all = scipy.stats.linregress(x, y)\n",
    "M = len(x)\n",
    "\n",
    "def update_plot(T):\n",
    "    [ax_.cla() for ax_ in ax.ravel()]\n",
    "    ax[0, 0].scatter(x, y, zorder=100, s=10)\n",
    "    np.random.seed(0)\n",
    "    param = np.zeros(shape=(T, 3))\n",
    "    lines = []\n",
    "    for t in range(T):\n",
    "        bootstrap_idx = np.random.choice(np.arange(len(x)), size=len(x), replace=True)\n",
    "        res = scipy.stats.linregress(x[bootstrap_idx], y[bootstrap_idx])\n",
    "        lines.append(x_plot*res.slope + res.intercept)\n",
    "        #ax[0, 0].plot(x_plot, x_plot*res.slope + res.intercept, alpha=0.05, c='k')\n",
    "        param[t, :] = [res.intercept, res.slope, res.rvalue]\n",
    "    ax[0, 0].plot(x_plot, x_plot*res_all.slope + res_all.intercept, alpha=1, c='r')\n",
    "    lines = np.stack(lines)\n",
    "    ax[0, 0].fill_between(x_plot, np.mean(lines, axis=0)-3*np.std(lines, axis=0), \n",
    "                          np.mean(lines, axis=0)+3*np.std(lines, axis=0), color='r', alpha=0.25)\n",
    "    hist_val, hist_lim, _ = ax[0, 1].hist(param[:, 2], bins=15, density=True)\n",
    "    ax[0, 1].plot([res_all.rvalue]*2, [0, np.max(hist_val)], c='r')\n",
    "    ax[0, 1].set_xlabel('r')\n",
    "    display(\"Intervalo de confianza al 95% de r {}\".format(np.percentile(param[:, 2], [2.5, 97.5])))\n",
    "    hist_val, hist_lim, _ = ax[1, 0].hist(param[:, 0], bins=15, density=True)\n",
    "    ax[1, 0].plot([res_all.intercept]*2, [0, np.max(hist_val)], c='r')\n",
    "    ax[1, 0].set_xlabel(r'$\\theta_0$')\n",
    "    display(\"Intervalo de confianza al 95% de theta_0 {}\".format(np.percentile(param[:, 0], [2.5, 97.5])))\n",
    "    hist_val, hist_lim, _ = ax[1, 1].hist(param[:, 1], bins=15, density=True)\n",
    "    ax[1, 1].plot([res_all.slope]*2, [0, np.max(hist_val)], c='r')\n",
    "    ax[1, 1].set_xlabel(r'$\\theta_1$')\n",
    "    display(\"Intervalo de confianza al 95% de theta_1 {}\".format(np.percentile(param[:, 1], [2.5, 97.5])))\n",
    "\n",
    "widgets.interact(update_plot, T=SelSlider_nice(options=[10, 20, 50, 100, 200, 500], value=50));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\theta_1=0$ no está en el intervalo de confianza por lo que podemos rechazar $H_0$\n",
    "- Notamos que $\\theta_0$ y $\\theta_1$ son asintóticamente normales. El supuesto del t-test estaba bien!\n",
    "- Si hubieramos hecho un t-test sobre $r$ no sería valido (no normalidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más sobre [*bootstrap* y regresión lineal](https://www.stat.cmu.edu/~cshalizi/402/lectures/08-bootstrap/lecture-08.pdf) [aquí](http://homepage.divms.uiowa.edu/~rdecook/stat3200/notes/bootstrap_4pp.pdf) y [acá](https://www.sagepub.com/sites/default/files/upm-binaries/21122_Chapter_21.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm Icecream.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Regresión lineal multivariada y mínimos cuadrados\n",
    "\n",
    "Queremos ajustar un modelo lineal de $N$ parámetros a un conjunto de $M$ observaciones ruidosas\n",
    "\n",
    "Podemos modelar \n",
    "\n",
    "$$\n",
    "y_i = \\Phi(x_i) \\theta + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "donde $\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\varepsilon^2)$\n",
    "\n",
    "Si asumimos que los datos son iid podemos escribir el logaritmo de la verosimilitud como\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\log p(y_1, y_2, \\ldots, y_M| \\theta) &= \\log \\prod_{i=1}^M p(y_i|\\theta) \\nonumber \\\\\n",
    "&=  -\\frac{1}{2} \\log(2\\pi\\sigma_\\varepsilon^2) M - \\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M (y_i - \\Phi(x_i) \\theta)^2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "el primer término no depende de $\\theta$ y el segundo es negativo\n",
    "\n",
    "El resultado de maximizar la log verosimilitud es equivalente a\n",
    "$$\n",
    "\\min_\\theta \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^M (y_i - \\Phi(x_i) \\theta)^2\n",
    "$$\n",
    "\n",
    "Que corresponde al problema de **mínimos cuadrados**\n",
    "\n",
    "#### Reflexionemos\n",
    "¿Qué estamos asumiendo cuando usamos mínimos cuadrados?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximum a posteriori (MAP)\n",
    "\n",
    "- Hasta ahora nos hemos enfocado en la verosimilitud $p(\\mathcal{D}|\\theta)$\n",
    "- En realidad lo que más nos interesa es el *posterior* $p(\\theta|\\mathcal{D})$\n",
    "\n",
    "Bayes nos dice que\n",
    "\n",
    "$$\n",
    "p(\\theta|\\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta)p(\\theta)}{p(\\mathcal{D})}\n",
    "$$\n",
    "\n",
    "El criterio MAP busca encontrar $\\theta$ que maximiza el posterior. \n",
    "\n",
    "Es decir la moda del posterior\n",
    "\n",
    "Usando el logaritmo para escribir\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg}\\max_\\theta \\log p(\\theta|\\mathcal{D}) \\nonumber \\\\\n",
    "&= \\text{arg}\\max_\\theta \\log p(\\mathcal{D}|\\theta) + \\log p(\\theta) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "donde omitimos la evidencia $p(\\mathcal{D})$ ya que no depende de $\\theta$\n",
    "\n",
    "- El criterio MAP consiste en maximizar la **verosimilitud** más el **prior**\n",
    "- Usamos el **prior** para agregar información sobre $\\theta$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo: Regresión  con MAP\n",
    "\n",
    "Quiero modelar una regresión de $N$ parámetros\n",
    "$$\n",
    "\\begin{align}\n",
    "y_i = & \\Phi(x_i) \\theta + \\varepsilon_i \\nonumber \\\\\n",
    "&\\varepsilon_i \\sim \\mathcal{N}(0, \\sigma_\\epsilon^2) \\nonumber \\\\\n",
    "&y_i|\\theta \\sim \\mathcal{N}(\\Phi(x_i) \\theta, \\sigma_\\epsilon^2) \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "es decir **verosimilitud** Gaussiana tal como antes\n",
    "\n",
    "Adicionalmente asumiremos que $p(\\theta) = \\mathcal{N}(0, \\sigma_\\theta^2)$, es decir **prior** Gaussiano\n",
    "\n",
    "El estimador MAP es \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat \\theta &= \\text{arg} \\max_\\theta -\\frac{1}{2} \\log(2\\pi\\sigma_\\varepsilon^2) M - \\frac{1}{2\\sigma_\\varepsilon^2} \\sum_{i=1}^M (y_i - \\Phi(x_i) \\theta)^2 -\\frac{1}{2} \\log(2\\pi \\sigma_\\theta^2) N - \\frac{1}{2\\sigma_\\theta^2} \\sum_{j=1}^N \\theta_j^2 \\nonumber \\\\\n",
    "&= \\text{arg} \\min_\\theta  \\sum_{i=1}^M (y_i - \\Phi(x_i) \\theta)^2 + \\frac{\\sigma_\\varepsilon^2}{\\sigma_\\theta^2} \\sum_{j=1}^N \\theta_j^2 \\nonumber \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Reflexión\n",
    "- Si hay mucho ruido ($\\sigma_\\varepsilon^2 \\to \\infty$) entonces ignoro la verosimilitud y le creo al prior\n",
    "- Si uso un prior que no me da información ($\\sigma_\\theta^2 \\to \\infty$) entonces lo ignoro y me enfoco en la verosimilitud\n",
    "\n",
    "#### Relación con lo visto sobre regularización\n",
    "\n",
    "- Si decimos $\\lambda = \\frac{\\sigma_\\varepsilon^2}{\\sigma_\\theta^2}$ entonces la formulación es equivalente a Mínimos cuadrados regularizado, Ridge Regressión, Tikhonov!\n",
    "- Si hubieramos asumido una distribución Laplaciana para $p(\\theta)$ hubieramos obtenido el [LASSO](https://en.wikipedia.org/wiki/Lasso_(statistics))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencia Bayesiana\n",
    "\n",
    "- Asumimos que $\\theta$ es una variable aleatoria y que tiene cierta distribución\n",
    "- Buscamos estimar el posterior completo $p(\\theta|\\mathcal{D})$\n",
    "- El problema es que $p(\\mathcal{D})$ es generalmente incalculable\n",
    "- Para proseguir tenemos tres opciones\n",
    "    1. Usar distribuciones que sean conjugadas\n",
    "    1. Usar una aproximación (inferencia variacional)\n",
    "    1. Aprender un sistema que se comporte como el posterior (Markov Chain Monte Carlo) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "\n",
    "temperatura, consumo = df[\"temperatura\"].values, df[\"consumo\"].values\n",
    "with pm.Model() as helados:\n",
    "    sigma = pm.HalfNormal('sigma', sd=10, shape=1)\n",
    "    theta0 = pm.Normal('intercept', mu=0, sd=10, shape=1)\n",
    "    theta1 = pm.Normal('slope', mu=0, sd=10, shape=1)\n",
    "    mu = temperatura*theta1 + theta0\n",
    "    x_observed = pm.Normal('x_obs', mu=mu, sd=sigma, observed=consumo)\n",
    "    trace = pm.sample(draws=500, tune=200, init='advi', n_init=30000, \n",
    "                      cores=4, chains=2, live_plot=False)\n",
    "display(pm.summary(trace))\n",
    "pm.traceplot(trace, figsize=(9, 6), combined=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más sobre estos temas en el [repositorio de INFO337](https://github.com/magister-informatica-uach/INFO337/blob/master/MCMC/lecture.ipynb) del magíster de informática y en [Ipython](https://ipython-books.github.io/73-getting-started-with-bayesian-methods/) [cookbook](https://ipython-books.github.io/77-fitting-a-bayesian-model-by-sampling-from-a-posterior-distribution-with-a-markov-chain-monte-carlo-method/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para leer y reflexionar: \n",
    "\n",
    "https://www.sciencedirect.com/science/article/pii/S0167715218300737"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUTURO\n",
    "\n",
    "Más sobre métodos de monte-carlo\n",
    "- https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50\n",
    "- https://skymind.ai/wiki/markov-chain-monte-carlo\n",
    "- https://help.xlstat.com/customer/en/portal/articles/2062457-which-statistical-test-should-you-use-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
